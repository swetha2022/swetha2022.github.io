<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data">
  <meta name="keywords" content="Vision-Language Model, Data Synthesis, Fine-Tuning, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>KALIE</title>
	<link rel="icon" type="image/png" href="images/moka_icon.png"/>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <!-- <div class="container is-fullhd"> -->
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Grace Tang*</a></span>,
            <span class="author-block">
              <a>Swetha Rajkumar*</a></span>,
            <span class="author-block">
              <a href="https://yifeizhou02.github.io">Yifei Zhou</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
            </span>
            <span class="author-block">
              <a href="https://kuanfang.github.io">Kuan Fang</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> denotes equal contribution, alphabetical order</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Berkeley AI Research, UC Berkeley</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.03174"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rail-berkeley/moka"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser"> -->
<!--   <div class="container is-max-desktop"> -->
<!--     <div class="hero-body"> -->
<!--       <img src="images/teaser.gif" -->
<!--                  class="interpolation-image" -->
<!--                  alt="Interpolation end reference image."/> -->
<!--       <h2 class="content has-text-justified"> -->
<!--         <b>MOKA</b> employs pre-trained vision-language models (GPT-4V) to predict <b>point-based affordance representations</b> for solving manipulation tasks in zero- or few-shot manners. By <b>annotating marks</b> (candidate points, grids, and captions) on RGB images, MOKA converts the motion generation problem into a series of visual question-answering problems that the VLM can address. -->
<!--       </h2> -->
<!--     </div> -->
<!--   </div> -->
<!-- </section> -->

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="videos/teaser.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <h2 class="subtitle has-text-justified">
        <b>MOKA</b> employs pre-trained vision-language models (GPT-4V) to predict <b>point-based affordance representations</b> for solving manipulation tasks in zero- or few-shot manners. By <b>annotating marks</b> (candidate points, grids, and captions) on RGB images, MOKA converts the motion generation problem into a series of visual question-answering problems that the VLM can address.
        </h2>
      </div>
    </div>
  </div>
</section>


<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-steve">-->
<!--          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/steve.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/chair-tp.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/shiba.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-fullbody">-->
<!--          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/fullbody.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-blueshirt">-->
<!--          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/blueshirt.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-mask">-->
<!--          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/mask.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-coffee">-->
<!--          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/coffee.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-toby">-->
<!--          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="videos/toby2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Building generalist robotic systems involves effectively endowing robots the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIEcontrols the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.            <!--   We present MOKA, an approach that employs VLMs' reasoning capabilities for robotic manipulation in open-vocabulary settings. -->
            <!--   At the heart of our approach is a compact point-based representation of affordance and motion that bridges -->
            <!--   the VLM's predictions on RGB images and the robot's motions in the physical world. -->
            <!-- [> </p> <] -->
            <!-- [> <p> <] -->
            <!--   By prompting a VLM pre-trained on internet-scale data, our approach predicts the affordances and generates the corresponding motions -->
            <!--   by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, -->
            <!--   we propose a visual prompting technique that annotates visual and textual marks on the raw images, converting the prediction -->
            <!--   of keypoints and waypoints into a visual question answering problem that is feasible for the VLM to solve. -->
            <!-- [> </p> <] -->
            <!-- [> <p> <] -->
            <!--   We evaluate and analyze MOKA's performance across a wide variety of manipulation tasks specified by -->
            <!--   free-form language descriptions, spanning across tool use, deformable body manipulation, object rearrangement, etc. -->
            <!--   MOKA can successfully achieve high success rates on all these tasks in zero-shot or few-shot and -->
            <!--   provide interpretable intermediate results for human users. -->
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <div class="container is-fullhd">
    <div class="my-block">
      <!-- <div class="columns is-centered has-text-centered"> -->
        <div class="column is-full-width">
          <h2 class="title is-3">Marking Open-vocabulary Keypoint Affordances (MOKA)</h2>

          <h2 class="content has-text-justified">
            MOKA employs VLMs in a <b>hierarchical visual prompting</b> framework, which converts the affordance reasoning problem into a series of visual question problems that are feasible for the VLM to address.
          </h2>

          <div>
            <img src="images/model.png"
                       class="interpolation-image"
                       alt="Interpolation end reference image."/>
          </div>

          <h2 class="content has-text-justified">
						On the <b>high level</b>, the VLM is prompted to decompose the free-form language description of the task into a sequence of subtasks and summarize the subtask information. 
						<!-- </br> -->
						On the <b>low level</b>, the VLM produces the point-based affordance representation based on the marked image. 
          </h2>

        <!-- </div> -->
      </div>
    </div>

    <div class="my-block">
      <!-- <div class="columns is-centered has-text-centered"> -->
        <div class="column is-full-width">
          <h2 class="title is-3">Point-based Affordance Representations</h2>
          <h2 class="content has-text-justified">
            To bridge the VLM’s predictions on 2D images and the robot’s motion in the physical world, we introduce a point-based affordance representation. Using a set of keypoints and waypoints, we can specify the robot’s motions for a wide range of tasks. 
          </h2>
          <div>
          <img src="images/affordances.png"
                     class="interpolation-image"
                     alt="Interpolation end reference image."
                     />
          </div>
        </div>
      <!-- </div> -->
    </div>

    <!-- <div class="container is-max-widescreen"> -->
    <div class="my-block">
    <!-- <div class="container is-fullhd"> -->

      <!-- <div class="rows"> -->
      <!-- <div class="container is-fullhd"> -->
      <div class="rows is-fullhd">
        <!-- <div class="column has-text-centered"> -->
        <h2 class="title is-3">Tasks</h2>
        
        <p class="content has-text-justified">
          Given free-form descriptions of the tasks, MOKA can effectively predict the point-based affordance representations and generates the desired motions.
        </p>
        
        <div class="rows">
          <div class="columns">
            <div class="column has-text-justified">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_0_0.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Move the eyeglasses onto the yellow cloth and use the brush to sweep the snack package to the right side of the table."
              </p>
              <p style="text-align:center">
								(Subtask 1)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_1_0.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Use the ultrasound cleaner to clean the metal watch. The unstrasound cleaner has no lid and can be turned on by pressing the red button."
              </p>
              <p style="text-align:center">
								(Subtask 1)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_a0.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Close the drawer."
              </p>
            </div>
          </div>

          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_0_1.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Move the eyeglasses onto the yellow cloth and use the brush to sweep the snack package to the right side of the table."
              </p>
              <p style="text-align:center">
								(Subtask 2)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_1_1.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Use the ultrasound cleaner to clean the metal watch. The unstrasound cleaner has no lid and can be turned on by pressing the red button."
              </p>
              <p style="text-align:center">
								(Subtask 2)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_a1.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Insert the pink roses into the vase."
              </p>
            </div>
          </div>

        <div class="rows">
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_2_0.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Make a gift box containing the perfurme bottle. Put some golden filler beneath the perfume."
              </p>
              <p style="text-align:center">
								(Subtask 1)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_3_0.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Unplug the charge cable and close the lid of the laptop."
              </p>
              <p style="text-align:center">
								(Subtask 1)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_a2.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Use the fur remover to remove the white fur ball on the sweater."
              </p>
            </div>
          </div>

          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_2_1.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Make a gift box containing the perfurme bottle. Put some golden filler beneath the perfume."
              </p>
              <p style="text-align:center">
								(Subtask 2)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_3_1.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Unplug the charge cable and close the lid of the laptop."
              </p>
              <p style="text-align:center">
								(Subtask 2)
              </p>
            </div>
        
            <div class="column has-text-centered">
              <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
                <source src="videos/moka_task_a3.mp4" type="video/mp4">
              </video>
              <p style="text-align:left">
                "Put the scissors in the hand."
              </p>
            </div>
          </div>

        </div>

      </div>
    </div>

		</br>
		</br>

    <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">Robustness Analysis</h2>
          <div>
          <h2 class="content has-text-justified">
            For the same task, MOKA's prediction is robust to variations of instructions, objects, and initial poses. Each column in the image uses the same language instruction and similar initial arrangements of objects. The two rows involve different objects.
          </h2>
					<div class="columns is-centered has-text-centered">
						<img src="images/robustness.png"
											 class="interpolation-image"
											 alt="Interpolation end reference image."
											 width="800px"
											 />
						</div>
					</div>
			</div>
    </div>

     <div class="my-block">
      <div class="column is-full-width">
        <h2 class="title is-3">Acknowledgement</h2>
        <div>
          <h2 class="content has-text-justified">
            Our system is built upon the <a href="https://droid-dataset.github.io/droid/hardware-setup/shopping-list.html">DROID hardware</a> setup with a Franka Research 3 (FR3) robot.
          </h2>
        </div>
      </div>
    </div>

    <!-- <div class="my-block"> -->
    <!--   <div class="columns is-centered has-text-centered"> -->
    <!--     <div class="column is-four-fifths"> -->
    <!--       <h2 class="title is-3">Tasks</h2> -->
    <!--       <div> -->
    <!--         <img src="images/tasks.png" class="interpolation-image" alt="Interpolation end reference image."/> -->
    <!--       </div> -->
    <!--       <h2 class="content has-text-justified"> -->
    <!--       <span class="dnerf">MOKA</span> is evaluated on a wide variety of manipulation tasks specified by free-form language descriptions, spanning across tool use, deformable body manipulation, object rearrangement, etc. -->
    <!--     </h2> -->
    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->
    <!--  -->
    <!-- <div class="my-block"> -->
    <!--   <div class="columns is-centered has-text-centered"> -->
    <!--     <div class="column is-full-width"> -->
    <!--       <h2 class="title is-3">Examples</h2> -->
    <!--       <div> -->
    <!--       <img src="images/examples.jpg" -->
    <!--                  class="interpolation-image" -->
    <!--                  alt="Interpolation end reference image."/> -->
    <!--       </div> -->
    <!--       <h2 class="content has-text-centered">The illustration examples of visual prompts, VLM's responses and robot execution results on the evaluation tasks we proposed. -->
    <!--     </h2> -->
    <!--     </div> -->
    <!--   </div> -->
    <!-- </div> -->


    <!-- Paper video. -->
<!--    TODO: replace video -->
  <!--   <div class="columns is-centered has-text-centered"> -->
  <!--     <div class="column is-four-fifths"> -->
  <!--       <h2 class="title is-3">Video</h2> -->
  <!--       <div class="publication-video"> -->
  <!--         <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" -->
  <!--                 frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
  <!--       </div> -->
  <!--       <h2 class="content has-text-centered"> -->
  <!--       For more results and details about <span class="dnerf">MOKA</span>, please check the videos. -->
  <!--     </h2> -->
  <!--     </div> -->
  <!--   </div> -->
  <!--   [>/ Paper video. <] -->
  <!-- </div> -->

</section>



<!--<section class="section" id="BibTeX">-->
<!-- TODO: update with arxiv bibtex -->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <pre><code>@article{park2021nerfies,-->
<!--  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},-->
<!--  title     = {Nerfies: Deformable Neural Radiance Fields},-->
<!--  journal   = {ICCV},-->
<!--  year      = {2021},-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from nerfie's <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
